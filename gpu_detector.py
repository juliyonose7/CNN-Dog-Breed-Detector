"""
GPU Detector Module for AMD DirectML on Windows.
This module handles automatic hardware detection and configuration
for PyTorch inference using AMD GPU acceleration via DirectML.
"""

import torch
import torch_directml


def setup_amd_gpu():
    """
    Configure AMD GPU for Windows using DirectML backend.
    
    Detects available hardware accelerators in priority order:
    1. DirectML (AMD GPUs on Windows)
    2. CUDA (NVIDIA GPUs)
    3. CPU (fallback)
    
    Returns:
        tuple: (torch.device, bool) - The configured device and GPU availability flag.
    """
    print("üîç Detecting available hardware...")
    
    # Check DirectML availability for AMD GPUs
    if torch_directml.is_available():
        device_count = torch_directml.device_count()
        print(f"‚úÖ DirectML available with {device_count} device(s)")
        
        # Enumerate all available DirectML devices
        for i in range(device_count):
            device = torch_directml.device(i)
            print(f"   Device {i}: {device}")
        
        # Select the primary DirectML device
        device = torch_directml.device()
        print(f"üöÄ Using AMD GPU with DirectML: {device}")
        return device, True
    
    # Fallback to CUDA if available (NVIDIA GPUs)
    elif torch.cuda.is_available():
        device = torch.device('cuda')
        print(f"üü° Using CUDA: {torch.cuda.get_device_name()}")
        return device, True
    
    # Final fallback to CPU
    else:
        device = torch.device('cpu')
        print("‚ö†Ô∏è  Using CPU (no GPU detected)")
        return device, False


def test_gpu_performance():
    """
    Benchmark GPU performance with matrix multiplication operations.
    
    Executes a standardized benchmark using 1024x1024 matrix multiplication
    to measure GPU throughput and compare against CPU baseline.
    
    Returns:
        bool: True if GPU benchmark completed successfully, False otherwise.
    """
    device, gpu_available = setup_amd_gpu()
    
    if not gpu_available:
        return False
    
    print("\nüß™ Testing GPU performance...")
    
    import time
    
    # Create test tensors on the target device
    size = 1024
    a = torch.randn(size, size).to(device)
    b = torch.randn(size, size).to(device)
    
    # Warmup iterations to stabilize GPU clock speeds
    for _ in range(10):
        c = torch.matmul(a, b)
    
    # Synchronize CUDA stream if applicable
    if hasattr(torch, 'cuda') and torch.cuda.is_available():
        torch.cuda.synchronize()
    
    # Measure execution time for benchmark iterations
    start_time = time.time()
    for _ in range(100):
        c = torch.matmul(a, b)
    
    # Ensure all operations complete before timing
    if hasattr(torch, 'cuda') and torch.cuda.is_available():
        torch.cuda.synchronize()
    
    end_time = time.time()
    
    # Calculate and display performance metrics
    total_time = end_time - start_time
    ops_per_sec = 100 / total_time
    
    print(f"‚úÖ Matrix multiplication {size}x{size}:")
    print(f"   Total time: {total_time:.3f}s")
    print(f"   Operations/sec: {ops_per_sec:.1f}")
    print(f"   GPU is {ops_per_sec/10:.1f}x faster than CPU baseline")
    
    return True


if __name__ == "__main__":
    test_gpu_performance()